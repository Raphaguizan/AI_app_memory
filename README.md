# ğŸ¤– Chatbot com HistÃ³rico de Mensagens e OtimizaÃ§Ã£o de Tokens

Este projeto implementa um chatbot inteligente baseado na **API Groq**, utilizando a **LangChain** para gerenciar o histÃ³rico de conversas e otimizar o uso de tokens.

## ğŸ“Œ Funcionalidades

âœ… **IntegraÃ§Ã£o com a API Groq** para processamento de linguagem natural  
âœ… **Gerenciamento de histÃ³rico de conversas** por sessÃ£o de usuÃ¡rio  
âœ… **OtimizaÃ§Ã£o de tokens** para reduzir consumo de memÃ³ria  
âœ… **Sistema de prompts dinÃ¢micos** para maior controle sobre as respostas  
âœ… **Pipeline de execuÃ§Ã£o otimizado** para melhor eficiÃªncia  

---

## ğŸ›  Tecnologias Utilizadas

- **Python 3.8+**  
- [LangChain](https://python.langchain.com/) - Framework para IA conversacional  
- [Groq API](https://groq.com/) - Modelo de IA para geraÃ§Ã£o de respostas  
- [python-dotenv](https://pypi.org/project/python-dotenv/) - Gerenciamento de variÃ¡veis de ambiente  

---

## ğŸš€ Como Configurar o Projeto

### 1ï¸âƒ£ Clone o repositÃ³rio  
```bash
git clone https://github.com/Raphaguizan/AI_app_memory.git
cd seu-repositorio
```

### 2ï¸âƒ£ Crie um ambiente virtual e ative  
```bash
python -m venv venv  
source venv/bin/activate  # Linux/Mac  
venv\Scripts\activate  # Windows  
```

### 3ï¸âƒ£ Instale as dependÃªncias  
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure a chave da API Groq  
Crie um arquivo `.env` na raiz do projeto e adicione sua chave da API:  
```env
GROQ_API_KEY=your_api_key_here
```

### 5ï¸âƒ£ Execute o chatbot  
```bash
python main.py
```

---

## ğŸ“ Exemplo de Uso  

```python
# Exemplo de interaÃ§Ã£o com o chatbot
response = chain.invoke(
    {
        "messages": messages + [HumanMessage(content="Qual Ã© o sorvete que eu gosto?")]
    }
)
print(response.content)
```

---

## ğŸ“– DocumentaÃ§Ã£o do CÃ³digo

### ğŸ“‚ Estrutura do Projeto
```
/seu-repositorio
â”‚â”€â”€ main.py               # Arquivo principal do chatbot
â”‚â”€â”€ requirements.txt      # DependÃªncias do projeto
â”‚â”€â”€ .env                 # Chaves da API (nÃ£o incluso no repositÃ³rio)
â”‚â”€â”€ README.md            # DocumentaÃ§Ã£o do projeto
```

### ğŸ“œ ExplicaÃ§Ã£o do CÃ³digo

#### ğŸ“Œ ImportaÃ§Ã£o de Bibliotecas
```python
import os
from dotenv import load_dotenv, find_dotenv
from langchain_groq import ChatGroq
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
```
O cÃ³digo importa bibliotecas necessÃ¡rias para interaÃ§Ã£o com a API Groq e gerenciamento de histÃ³rico de mensagens.

#### ğŸ“Œ ConfiguraÃ§Ã£o do Ambiente
```python
load_dotenv(find_dotenv())
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
```
Carrega as variÃ¡veis de ambiente do arquivo `.env` para acessar a API.

#### ğŸ“Œ InicializaÃ§Ã£o do Modelo de IA
```python
llm = ChatGroq(
    model="gemma2-9b-it",
    groq_api_key=GROQ_API_KEY,
    temperature=1
)
```
Cria uma instÃ¢ncia do modelo **Groq AI** para processar mensagens.

#### ğŸ“Œ Gerenciamento do HistÃ³rico de Conversas
```python
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]
```
Cria e armazena histÃ³ricos de mensagens por sessÃ£o.

#### ğŸ“Œ ConfiguraÃ§Ã£o do Pipeline de ExecuÃ§Ã£o
```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© um assistente Ãºtil. Responda todas as perguntas com precisÃ£o."),
    MessagesPlaceholder(variable_name="messages")
])
```
Define um **template de prompt** para estruturar mensagens enviadas ao modelo.

#### ğŸ“Œ OtimizaÃ§Ã£o do Uso de Tokens
```python
trimmer = trim_messages(
    max_tokens=45,
    strategy="last",
    token_counter=llm,
    include_system=True,
    allow_partial=False,
    start_on="human"
)
```
Implementa um **limitador de tokens** para evitar alto consumo de memÃ³ria.

#### ğŸ“Œ ExecuÃ§Ã£o da IA
```python
chain = (
    RunnablePassthrough.assign(messages=itemgetter("messages") | trimmer)
    | prompt
    | llm
)
```
Cria um **pipeline de execuÃ§Ã£o** para conectar as entradas do usuÃ¡rio ao modelo de IA.

#### ğŸ“Œ Exemplo de InteraÃ§Ã£o
```python
response = chain.invoke(
    {
        "messages": messages + [HumanMessage(content="Qual Ã© o sorvete que eu gosto?")]
    }
)
print(response.content)
```
Executa uma consulta ao chatbot e exibe a resposta da IA.

---

## ğŸ“œ LicenÃ§a  

Este projeto estÃ¡ licenciado sob a **MIT License**.  

---

ğŸ’¡ **DÃºvidas ou sugestÃµes?** Sinta-se Ã  vontade para abrir uma _issue_ ou enviar um _pull request_.  

